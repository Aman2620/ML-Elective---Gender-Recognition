{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1e3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc714bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('balanced-all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152a3c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/cv-other-train/sample-069205.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/cv-valid-train/sample-063134.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/cv-other-train/sample-080873.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/cv-other-train/sample-105595.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/cv-valid-train/sample-144613.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  gender\n",
       "0  data/cv-other-train/sample-069205.npy  female\n",
       "1  data/cv-valid-train/sample-063134.npy  female\n",
       "2  data/cv-other-train/sample-080873.npy  female\n",
       "3  data/cv-other-train/sample-105595.npy  female\n",
       "4  data/cv-valid-train/sample-144613.npy  female"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ae37fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  66938\n",
      "Total male samples:  33469\n",
      "Total female samples:  33469\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of samples: \",len(df))\n",
    "print(\"Total male samples: \", len(df[df['gender']=='male']))\n",
    "print(\"Total female samples: \", len(df[df['gender']=='female']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c5ee10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189a8a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='gender', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(15, 5)})\n",
    "sns.countplot(x=\"gender\", \n",
    "        data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d89408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\n",
    "    \"male\": 1,\n",
    "    \"female\": 0\n",
    "}\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/labels.npy\"):\n",
    "        X = np.load(\"results/features.npy\")\n",
    "        y = np.load(\"results/labels.npy\")\n",
    "        return X, y\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(\"balanced-all.csv\")\n",
    "    # get total samples\n",
    "    n_samples = len(df)\n",
    "    # get total male samples\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    # get total female samples\n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros((n_samples, vector_length))\n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "    np.save(\"results/features\", X)\n",
    "    np.save(\"results/labels\", y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c0ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46c6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35304c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5921d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vector_length=128):\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e78265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2548b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "848/848 [==============================] - 4s 3ms/step - loss: 0.5729 - accuracy: 0.7609 - val_loss: 0.3929 - val_accuracy: 0.8413\n",
      "Epoch 2/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.4205 - accuracy: 0.8325 - val_loss: 0.3323 - val_accuracy: 0.8667\n",
      "Epoch 3/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3855 - accuracy: 0.8484 - val_loss: 0.3164 - val_accuracy: 0.8699\n",
      "Epoch 4/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3621 - accuracy: 0.8594 - val_loss: 0.3134 - val_accuracy: 0.8727\n",
      "Epoch 5/100\n",
      "848/848 [==============================] - 3s 4ms/step - loss: 0.3467 - accuracy: 0.8664 - val_loss: 0.2959 - val_accuracy: 0.8873\n",
      "Epoch 6/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3405 - accuracy: 0.8712 - val_loss: 0.2961 - val_accuracy: 0.8866\n",
      "Epoch 7/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3282 - accuracy: 0.8746 - val_loss: 0.2811 - val_accuracy: 0.8948\n",
      "Epoch 8/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3219 - accuracy: 0.8760 - val_loss: 0.2700 - val_accuracy: 0.8966\n",
      "Epoch 9/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3149 - accuracy: 0.8807 - val_loss: 0.2763 - val_accuracy: 0.9012\n",
      "Epoch 10/100\n",
      "848/848 [==============================] - 3s 4ms/step - loss: 0.3136 - accuracy: 0.8815 - val_loss: 0.2697 - val_accuracy: 0.8973\n",
      "Epoch 11/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.3045 - accuracy: 0.8845 - val_loss: 0.2571 - val_accuracy: 0.9019\n",
      "Epoch 12/100\n",
      "848/848 [==============================] - 4s 4ms/step - loss: 0.2993 - accuracy: 0.8877 - val_loss: 0.2575 - val_accuracy: 0.8959\n",
      "Epoch 13/100\n",
      "848/848 [==============================] - 3s 4ms/step - loss: 0.2967 - accuracy: 0.8886 - val_loss: 0.2584 - val_accuracy: 0.9077\n",
      "Epoch 14/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2964 - accuracy: 0.8890 - val_loss: 0.2548 - val_accuracy: 0.9061\n",
      "Epoch 15/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2911 - accuracy: 0.8904 - val_loss: 0.2521 - val_accuracy: 0.9072\n",
      "Epoch 16/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2890 - accuracy: 0.8928 - val_loss: 0.2465 - val_accuracy: 0.9071\n",
      "Epoch 17/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2838 - accuracy: 0.8933 - val_loss: 0.2447 - val_accuracy: 0.9052\n",
      "Epoch 18/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2835 - accuracy: 0.8936 - val_loss: 0.2642 - val_accuracy: 0.9044\n",
      "Epoch 19/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2814 - accuracy: 0.8947 - val_loss: 0.2445 - val_accuracy: 0.9057\n",
      "Epoch 20/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2808 - accuracy: 0.8964 - val_loss: 0.2490 - val_accuracy: 0.9122\n",
      "Epoch 21/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2780 - accuracy: 0.8962 - val_loss: 0.2396 - val_accuracy: 0.9130\n",
      "Epoch 22/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2765 - accuracy: 0.8957 - val_loss: 0.2449 - val_accuracy: 0.9051\n",
      "Epoch 23/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2729 - accuracy: 0.9001 - val_loss: 0.2436 - val_accuracy: 0.9092\n",
      "Epoch 24/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2714 - accuracy: 0.8989 - val_loss: 0.2358 - val_accuracy: 0.9139\n",
      "Epoch 25/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2678 - accuracy: 0.8989 - val_loss: 0.2347 - val_accuracy: 0.9107\n",
      "Epoch 26/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2707 - accuracy: 0.9005 - val_loss: 0.2568 - val_accuracy: 0.9054\n",
      "Epoch 27/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2686 - accuracy: 0.9002 - val_loss: 0.2347 - val_accuracy: 0.9147\n",
      "Epoch 28/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2741 - accuracy: 0.8986 - val_loss: 0.2373 - val_accuracy: 0.9090\n",
      "Epoch 29/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2728 - accuracy: 0.9007 - val_loss: 0.2255 - val_accuracy: 0.9165\n",
      "Epoch 30/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2628 - accuracy: 0.9027 - val_loss: 0.2345 - val_accuracy: 0.9132\n",
      "Epoch 31/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2643 - accuracy: 0.9033 - val_loss: 0.2340 - val_accuracy: 0.9149\n",
      "Epoch 32/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2609 - accuracy: 0.9040 - val_loss: 0.2238 - val_accuracy: 0.9195\n",
      "Epoch 33/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2609 - accuracy: 0.9022 - val_loss: 0.2354 - val_accuracy: 0.9125\n",
      "Epoch 34/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2616 - accuracy: 0.9025 - val_loss: 0.2238 - val_accuracy: 0.9157\n",
      "Epoch 35/100\n",
      "848/848 [==============================] - 4s 4ms/step - loss: 0.2558 - accuracy: 0.9044 - val_loss: 0.2234 - val_accuracy: 0.9202\n",
      "Epoch 36/100\n",
      "848/848 [==============================] - 3s 4ms/step - loss: 0.2566 - accuracy: 0.9045 - val_loss: 0.2302 - val_accuracy: 0.9160\n",
      "Epoch 37/100\n",
      "848/848 [==============================] - 3s 4ms/step - loss: 0.2612 - accuracy: 0.9043 - val_loss: 0.2310 - val_accuracy: 0.9152\n",
      "Epoch 38/100\n",
      "848/848 [==============================] - 3s 4ms/step - loss: 0.2670 - accuracy: 0.9035 - val_loss: 0.2279 - val_accuracy: 0.9147\n",
      "Epoch 39/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2577 - accuracy: 0.9046 - val_loss: 0.2294 - val_accuracy: 0.9154\n",
      "Epoch 40/100\n",
      "848/848 [==============================] - 3s 3ms/step - loss: 0.2590 - accuracy: 0.9050 - val_loss: 0.2242 - val_accuracy: 0.9147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25a95274f70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b859c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model using 6694 samples...\n",
      "Loss: 0.2265\n",
      "Accuracy: 91.66%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b83a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"results/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2790c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyaudio\n",
    "# import os\n",
    "# import wave\n",
    "# from array import array\n",
    "# from sys import byteorder\n",
    "# from struct import pack\n",
    "\n",
    "# def record(outputfile):\n",
    "#     CHUNK = 1024\n",
    "#     FORMAT = pyaudio.paInt16\n",
    "#     CHANNELS = 2\n",
    "#     RATE = 44100\n",
    "#     RECORD_SECONDS = 10\n",
    "\n",
    "#     p = pyaudio.PyAudio()\n",
    "#     stream = p.open(format=FORMAT,\n",
    "#     channels=CHANNELS,\n",
    "#     rate=RATE,\n",
    "#     input=True,\n",
    "#     frames_per_buffer=CHUNK)\n",
    "\n",
    "#     print(\"Recording......\")\n",
    "\n",
    "#     frames = []\n",
    "\n",
    "#     for i in range(0,int(RATE/CHUNK*RECORD_SECONDS)):\n",
    "#         data = stream.read(CHUNK)\n",
    "#         frames.append(data)\n",
    "    \n",
    "#     print(\"Done Recording...\")\n",
    "\n",
    "#     stream.stop_stream()\n",
    "#     stream.close()\n",
    "#     p.terminate()\n",
    "\n",
    "#     wf = wave.open(outputfile,'wb')\n",
    "#     wf.setnchannels(CHANNELS)\n",
    "#     wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "#     wf.setframerate(RATE)\n",
    "#     wf.writeframes(b''.join(frames))\n",
    "#     wf.close()\n",
    "\n",
    "# # record('output1.wav')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a67e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be0a58f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 84ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_868\\1908479891.py:29: FutureWarning: Pass y=[-3.5229730e-06 -9.4623101e-06 -3.3905148e-06 ...  3.0257091e-02\n",
      "  3.3409148e-02  4.0582184e-02] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: male\n",
      "Probabilities::: Male: 96.24%    Female: 3.76%\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "model = create_model()\n",
    "\n",
    "model.load_weights(\"results/model.h5\")\n",
    "# extract features and reshape it\n",
    "features = extract_feature('output3.wav', mel=True).reshape(1, -1)\n",
    "# predict the gender\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "# show the result\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe110df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
